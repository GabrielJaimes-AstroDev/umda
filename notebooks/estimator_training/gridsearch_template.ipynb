{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd038815272606d4e3b04bbd3a96dc4b085d5a6c2ed1c5ead0b1b607595242e786b",
   "display_name": "Python 3.7.9 64-bit ('umda': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "38815272606d4e3b04bbd3a96dc4b085d5a6c2ed1c5ead0b1b607595242e786b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from joblib import load\n",
    "from ruamel.yaml import YAML\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from umda import paths\n",
    "from umda.data import load_data, load_pipeline\n",
    "from umda import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1215677\n",
    "normalize = False\n",
    "mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.default_rng(seed)\n",
    "\n",
    "full_X, full_cluster_ids, tmc1_df = load_data()\n",
    "embedder = load_pipeline()\n",
    "\n",
    "tmc1_X = np.vstack([embedder.vectorize(smi) for smi in tmc1_df[\"SMILES\"]])\n",
    "tmc1_y = np.log10(tmc1_df[\"Column density (cm^-2)\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_hparams.yml\") as read_file:\n",
    "    hparams = YAML().load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_kernel = kernels.ConstantKernel() * \\\n",
    "    kernels.RBF(3.0, (1e-1, 10.0)) + \\\n",
    "    kernels.RationalQuadratic(200.0, 20.0, alpha_bounds=(1e-3, 5e2), length_scale_bounds=(50.0, 1e4)) * \\\n",
    "        kernels.ConstantKernel() + kernels.ConstantKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = {\n",
    "    \"linear_regression\": LinearRegression(),\n",
    "    \"ridge\": Ridge(),\n",
    "    \"br\": BayesianRidge(),\n",
    "    \"svr\": SVR(),\n",
    "    \"knn\": KNeighborsRegressor(),\n",
    "    \"rfr\": RandomForestRegressor(random_state=seed),\n",
    "    \"gbr\": GradientBoostingRegressor(random_state=seed),\n",
    "    \"gpr\": GaussianProcessRegressor(\n",
    "        kernel=gp_kernel, random_state=seed\n",
    "    )\n",
    "}\n",
    "\n",
    "models = {key: training.compose_model(value, normalize) for key, value in base_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized workflow\n",
    "model_results = dict()\n",
    "best_models = dict()\n",
    "cv_results = dict()\n",
    "for name in models.keys():\n",
    "    model = models.get(name)\n",
    "    print(f\"Working on {name} now.\")\n",
    "    hparam = hparams.get(name)\n",
    "    cv_grid = training.grid_cv_search((tmc1_X, tmc1_y), model, hparam, seed, verbose=1, n_splits=20, scoring=\"neg_mean_squared_error\")\n",
    "    print(f\"Model: {name} best CV score: {cv_grid.best_score_:.4e}\")\n",
    "    # \n",
    "    best_estimator, best_train, best_test, best_performance, best_index, log = training.standardized_fit_test((tmc1_X, tmc1_y), model, cv_grid.best_params_, seed, n_splits=20)\n",
    "    model_results[name] = log\n",
    "    best_models[name] = best_estimator\n",
    "    cv_results[name] = cv_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the cross-validation results\n",
    "for name in models.keys():\n",
    "    df = pd.DataFrame(cv_results[name].cv_results_)\n",
    "    keys = [\"mean_test_score\", \"rank_test_score\"]\n",
    "    keys.extend([key for key in df.keys() if \"param_\" in key])\n",
    "    df = df[keys]\n",
    "    # sort and reset the indices\n",
    "    df.sort_values(\"rank_test_score\", ascending=True, inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # dump to CSV file\n",
    "    if normalize:\n",
    "        flags = \"norm\"\n",
    "    else:\n",
    "        flags = \"unnorm\"\n",
    "    if mask:\n",
    "        flags += \"_mask\"\n",
    "    else:\n",
    "        flags += \"_nomask\"\n",
    "    df.to_csv(f\"outputs/grid_search/{name}_{flags}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"ridge\"].cv_results_.keys()"
   ]
  },
  {
   "source": [
    "## Exporting the hyperparameter optimization results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect up the dictionaries for best parameters\n",
    "best_param_dict = dict()\n",
    "for name in models.keys():\n",
    "    best_param_dict[name] = cv_results[name].best_params_\n",
    "with open(\"outputs/grid_search/optimized_hparams.yml\", \"w+\") as write_file:\n",
    "    YAML().dump(best_param_dict, write_file)"
   ]
  },
  {
   "source": [
    "## Writing the training reports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_splits = dict()\n",
    "\n",
    "for name, log in model_results.items():\n",
    "    df = pd.DataFrame(log).sort_values([\"performance\", \"r2\"], ascending=[True, False])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.to_csv(f\"outputs/grid_search/{name}_training_report.csv\", index=False)\n",
    "    best_splits[name] = (df.iloc[0][\"train_index\"], df.iloc[0][\"test_index\"])"
   ]
  },
  {
   "source": [
    "## Making an overview plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(hparams)\n",
    "formatted_names = {key: key.upper() for key in models.keys()}\n",
    "formatted_names[\"linear_regression\"] = \"LR\"\n",
    "formatted_names[\"ridge\"] = \"RR\"\n",
    "\n",
    "fig, axarray = plt.subplots(2, num_models // 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "for model_name, ax in zip(models.keys(), axarray.flatten()):\n",
    "    model = best_models.get(model_name)\n",
    "    train_split, test_split = best_splits.get(model_name)\n",
    "    # draw the ideal curve\n",
    "    ax.plot(np.arange(10, 16), np.arange(10, 16), ls=\"--\", alpha=0.4, color=\"k\")\n",
    "    ax.scatter(tmc1_y[train_split], model.predict(tmc1_X[train_split]), c=\"#6B9A9B\", label=\"Train\", s=10,)\n",
    "    ax.scatter(tmc1_y[test_split], model.predict(tmc1_X[test_split]), c=\"#E6AD39\", label=\"Holdout\", s=10,)\n",
    "    r2 = r2_score(tmc1_y, model.predict(tmc1_X))\n",
    "    ax.set(xlim=[10, 15], ylim=(10, 15))\n",
    "    real_name = formatted_names.get(model_name)\n",
    "    ax.set_title(f\"{real_name} - $R^2$: {r2:1.2f}\", loc=\"left\")\n",
    "    if model_name == \"linear_regression\":\n",
    "        ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "source": [
    "## Data importance estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_importance_estimation(estimator, data, seed: int, n_splits: int = 500):\n",
    "    X, y = data\n",
    "    splitter = ShuffleSplit(n_splits, test_size=0.2, random_state=seed)\n",
    "    log = list()\n",
    "    weights = np.ones((n_splits, y.size))\n",
    "    test_errors = list()\n",
    "    for split_index, (train_index, test_index) in enumerate(splitter.split(X, y)):\n",
    "        train_X, test_X, train_y, test_y = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        result = estimator.fit(train_X, train_y)\n",
    "        # compute the mean squared error\n",
    "        train_error = mean_squared_error(train_y, result.predict(train_X))\n",
    "        test_error = mean_squared_error(test_y, result.predict(test_X))\n",
    "        log.append(\n",
    "            {\"train_error\": train_error, \"test_error\": test_error, \"train_index\": train_index, \"test_index\": test_index}\n",
    "        )\n",
    "        test_errors.append(test_error)\n",
    "        weights[split_index, test_index] = 0.\n",
    "    # reshape so we can do matrix multiplication\n",
    "    test_errors = np.asarray(test_errors)[:,None]\n",
    "    molecule_weights = (weights * test_errors).std(axis=0)\n",
    "    molecule_weights /= np.min(molecule_weights)\n",
    "    return log, molecule_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_log, weights = bootstrap_importance_estimation(best_models[\"ridge\"], (tmc1_X, tmc1_y) ,seed, n_splits=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(tmc1_X, tmc1_y, n_samples=500, random_state=seed)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}